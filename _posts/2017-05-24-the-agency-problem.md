---
layout: post
title: "The Agency Problem"
---

As much talk as there has been on artificial general intelligence (AGI), I feel like we're still missing a major component. In particular, we're missing a sense of agency. One might believe, based on all the buzz, that some meaning AI is right around the corner. But, as far as I can tell, we have made no progress toward agency. By "agency", I mean embuing AI with the sense that it is an agent able to make decisions of it own. Able to have its own thoughts. None of the fancy AI's we'll seen have taken so much as a step in that direction. Sure, DeepBlue can win at chess, but does it want to? Does it care? Does it have any reason to get out of bed in the morning? No, it doesn't. And we're not even making progress in this direction.

Sure, you can allocate a bit of memory in a computer and tell it that that's the computers "score" and it "wants" to maximize it. It may find some clever optimization path towards doing that. It may make the same chess moves as if it cared a lot, but, under the hood, it doesn't actually care at all.

Agency might matter quite a lot. Or, at least, the ability to detect it on others. For example, in self-driving cars, does a car need to know if another entity has agency? A scarecrows won't walk in front of traffic, but a human might. (However, a scarecrow could still fall on the road, and the car would have to avoid that, so maybe it doesn't matter.) There's a difference between a dog on leash and dog not on leash. But is the car going to know this? Does it just learn it through statistical learning?

Also, how much closer have we gotten to making computers understand object permanence, or cause and effect? Zero
