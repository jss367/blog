ai ethics

## Intro


Practitioners need to wake up. Ai is leaving a state of childhood and entering a state of maturity. This is going to be regulated. It's our job to make sure it's done with understand and care


### Goal fo the post

ai ethics
at the end of these posts, i want to come out for or against things:
facial recognition
- other things...
- what capabilities governments should have
- what capabilities  private companies should have

I don't want to end with just "let's do common-sense reform". I want to get specific.

I think it's important to have an idea of what would a fix look like. Sure, sometimes just pointing out a problem is useful, but in many cases, and specifcally in these because they're going to have to many problems and solutions are not going to be straightforward.

It's a bit like saying there's a problem with rascism in the world. THere's just not much additional value in pointing it out. We need to be at the solution phase.



face recognition
- creepy things like taking pictures of people without their consent

identify protesters, then try to find them doing something illegal
- should it be banned against peaceful protest

8 out of 42 were verifiably correct... what does verifiably mean in this context?
https://www.bbc.com/news/uk-51237665

use the image of the crying woman yelling at  the cat labeled as a dog as an example for how you used to think of it.

clearview.ai uses info they scrapped... probably against terms of service

let's talk about how it fails, but first let's talk about whether we would want it or not if it didn't fail

given the falliability of law enforcement, how much power should they have?

these videos of people designing themselves aren't practical. and they probably won't work for long. whether they work depends on the actual sensor being used and a whole lot of details. it's not scalable. it's not the solution




different across genders


do people want the probability of success to be higher

if not, then that's the problem


What to do with gender in-specific pronouns?

Google has given both options, which is a good solution

Originally they didn't




## Ground Understanding

#### Facial Recognition Is EASY

one thing i want people to understand is that this tech is easy. it's really really easy. now that the hard work is done anyone with basic coding skills should be able to get a basic facial recognition prototype done over the weekend. that's how easy it is.

This has implications:
1. You're not going to stop the technology from proliferating. These are nuclear weapons that require centrifuges. Anyone with a laptop (or even a Raspberry Pi) can do this.

it's not just about big companies not doing it






Could we do something complex?
There are ways to teach a GAN to find and remove racism from algorithms, but there's not way to get the public to understand what a GAN is.



What questions should we ask:
Did your parents go to jail? Do your friends do drugs?
Are your parents divorced?


It's not clear how the data got it so wrong? These models don't seem that complex, so why aren't they better? Should they all be open source? Should the training data all be open source?


It's WAY to easy for humans who don't understand these algorithms to think they are capable of more than they are. For example, when you read an email, you can be confident that that is what the sender sent. The chance is a word got changed in the email process is basically 0. So people trust computers. But the same does not apply to predictive algorithms. When 538 says there's a 68? percent change Hillary will when, or there's a 50% chance this person will commit another crime, there are TWO differences

The first is that they are probabilistic, even if they were perfect.

The second is that they're not perfect, and aren't going to be any time soon.


# Should algos be held to a higher standard?

Yes - they are used at incredible scale. There should be careful reviews. And also appeals and the like



The onous isn't just on the algorithm creator though. It's on the people using and and implementing it and selling it. The entire chain needs to be involved in these processes. The person who knows the data and wrote the algorithm (and anyone worth their salt who's writting a production-level algorithm will become intimately familiar with the data) is best placed to guide it's usage. So those other people must not firewall that person off. Sales people have to make sure to make that person avilable to end customers. And ensure that that person understands how end customers are using the data.








What if conspiracy theory videos are popular? What if an algo finds that something is dumb (without understanding at all what they're doing) and will accept any video. So it jsut sends them crazier and crazier stuff because they keep clicking. How do you fix this? I have worked with elderly people who I felt like were taken advantage of in a photography store and in buying a laptop. They have all these features they couldn't use and didn't understand and pieces of equipment that made their lives harder.
Could the algo cluster videos and find conspiracy theories? Could the humans label these?



# How to prevent runaway feedback mechanisms?
If you recommend a meetup to more men, more men will join, then you'll recommend it more and more... how to break this cycle?


# Things could be made to help or break humans

Cars used it be less safe and people thought it was the drivers. Yes, it was, but you could still add some safety features.
The intersection below work absolutely breaks humans




articles:

https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing



o bir doktor. o bir hemşire. o bir doktor. o bir hemşire. o bir tamircidir. o bir tamircidir. o bir profesör. o bir doktor.



## AI opens up new possibilities

- like other technology, cloning...


## AI makes things easy


## There's a lot going on that we've never really answered

Is it OK to post someone else's picture online without their consent? People do it all the time, so I guess we decided it's OK (in the way that societies decide things)... What about a slightly embarassing picture? What about a REALLY embarrassing picture? Or a "you could get fired for this" picture? 
OK, then what about face swaps? Is it OK if I do a face swap and then post a picture? AI makes this easy. I could do a face swap and make it look like I'm hanging out with a celebrity (maybe face swap in Lassie?)

You could do this before. But AI makes it easy. You used to need to go frame by frame, but not any more... Was it OK before? Is it OK now?


responsibility:

no one feels responsible. First, this happens *everywhere*. I've worked on projects that didn't go well and the person who was solely responisble for it didn't feel responsible. OK, but this is a case where it's even worse with machine learning. In machine learning the engineer can claim that it's the data that made the decision, which defuses the blame. This is not acceptable.


The media has been horrible in this space. Their understanding of AI is not much more than knowing that articles with AI in the title get more clicks and they get an excuse to use a stock photo from Terminator.


Face ID - exact same thing that happens at the grocery store. They basically say they'll pay us for our shopping information. They don't use face id because it creeps people out.



i used to think it was such a joke that a cat detector would detect a dog. but i realize that i was misunderstanding the trust people put in computer systems. i was wrong, this is a real problem. for me, pouring deep into failed results from machine learning, when I see a wrong prediction, I think "Yea, I know... I'm trying to fix it. This stuff is still not very good". I thought it was a joke. But now I realize I was wrong. Because other people don't take it as a joke.When they see a computer print "likely repeat offender", they think it's true. they don't realize how nonsense it is.
ai ethics - again, this is software engineering, but remember that this has big consequences. some of the results show thatthis is just garbage software engineering. many times that's not a big deal - some app dies and i have to restart it. but we have to clear-eyed about the impact we are having.i read some studies, and i just think. well, these people suck at basic machine learning. the problems are not difficult to test. (by the i mean there are basic statistical tests that should have caught these issues).Some of it is going to the cheapest bidder. What can you do if another company has no idea what they're doing and underbids you. You're responsible - you knew how to test the product - and you priced it accordingly. but you lost to someone who clearly had no idea what they were doing.that means the buyer must be smarter and the seller must realize the importance of their actions. a buyer should know, when buying a piece of custom software, you need to know a bit about what you're doing. this isn't like buying a car. this is custom stuff.- these are used at scale -> they have more impact

Runaway feedback loops - maybe only redheads like ice cream. so you advertise it to them.- predictive policing is absolutely going to have this problem.

accountability?accountability is tricky. when something goes wrong, it is often because a multitude of small mistakes.an ai practitioner builds an algo for a certain distribution, and it's doesn't generalize well. perhaps they could have made a better algo. or maybe they could have 

imagine a cat and dog detector and you have a line between them. what if something is far away from the line? what if it doesn't match the particular distribution?- there is no way for the model to tell you unless you give it that option.

there can be true bugsbad dataincorrect assumptionsincorrect extrapolation (by humans or by the model because it doesn't have a chance)

people blame the algorithm, but I think it's going to find a way to optimize. The question is should it optimize(link to capitalism everywhere blog)?
for the do we need capitalism everywhere blog:
an AI could find a way to make the most money off people while doing the least
https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsyTo me, this is an entirely predictable result. If this wasn't the point, then what was the point? Was this AI going to decide that what it really wanted to do was help people so it came up with a different algo? That's the nonsense of TV. Algos don't do that. You're solving an optimization problem. Be realistic.
Here's how I read this story: A company decided to maximize their profits from providing health care, so they used an algorithm called a neural network to optimize the amount they charged while reducing what they provided. And then the media says "Bad algorithm!!! AI so scary!!!!" This is fucking nonsense to blame the algorithm. But it definitely results in a lot of clicks, so here we are. (First, provide a simple to understand version of an optimization algorithm).(Although in this particular case it looks like there was a software bug)
- so this story goes to knowing how important your work is.
ythefuck was the person fired?- this boils down to the extra trust thing. it seems laughable to me that anyone would trust these algorithms beyond their capability, but they do. 
it's kind of like the lap coat phonemenon, where you see people wearing labcoats on TV to bulk their credibility. a computer has ultimate credibility for some people.


no one feels responsible, and for good reason. that's why it's such a difficult problem that requires attention.Data cleaning is the responsibility of the ml practitioner. You are responsible for thouroughly understanding your data. I spend most of my time messing with data. It's noisy and downright wrong many times. I run sanity check after sanity check on it. Even for a low consequence application (e.g. a hot dog not hot dog phone app), I would delve deeply into the data. As the importance of your work increases, so must your knowledge of the data. but the baseline is still really high. I think most people would be surprised by how much of the time of machine learning engineers consists of data cleaning.Here's an example of data not being cleaned well enough: https://www.latimes.com/local/lanow/la-me-ln-calgangs-audit-20160811-snap-story.html 
THE ML ENGINEER IS RESPONSIBLE FOR THE DATA THAT GOES INTO THE ALGORITHM, UNLESS ANOTHER ML ENGINEER IN YOUR COMPANY VOUCHES FOR IT

Have an honest debate - if you criticize a facial recognition algorithm for being worse with black people, then imagine it weren't. would you support it then? if not, you're being disingenuous by saying you don't support it because of the racial reason. you don't like it at all, so be honest and make *that* argument.
some of the code is complete garbage: https://www.theverge.com/2018/7/26/17615634/amazon-rekognition-aclu-mug-shot-congress-facial-recognition

we can have ai solutions for some things, like gans that find if you're penalizing the word 'woman' (https://www.theverge.com/2018/10/10/17958784/ai-recruiting-tool-bias-amazon-report), or zip codes associated with black people. but in general, i think we put too much emphasis on 

does facebook advertise to terrorists?- youtube will be tempted to put people down rabbit holes
it's important to focus. let's focus on ai. there is a lot of criticism with capitalism and society and other things, but i find that i only make progress thinking through an issue when i focus clearly on it. so the focus of this post is on ai ethics. * note: ai used in the media way, not in the real way. whether ai can become conscious is far off from today.

what to hold companies accountable for? there will be mistakes. some of these seem obvious though
- let's say fb does advertise  to terrorists... what to do about it?- or if youtube turns a bunch of people into nutjobs


i find it's too easy to just say "we need common sense reform". everyone celebrates that message but it's basically an empty statement. i'll try to be more specific. I don't like when conclusions are so vague that 100% of people will agree with them.

just like i'm a software engineer and dont want software involved in our elections(link)

## Shortcut learning

shortcut learning is going to be a problem.

they will find the shortcut

there is still research to do about this. i don't want to get too far into the details but perhaps these problems can be reduced by augmentation. i don't think very highly of using adversarial training, but we'll see. * because it's so unrealistic

think through your distribution - it's easier to think clearly when you separate the logic from the emotional aspects, then, once you've got the logic sound, reapply it to the emotional situation. you have to be careful in doing this. don't misunderstand the situation. so we'll talk about cat detectors. did you include hairless cats in your training set?



## Rate Limits

Ai ethics: rate limiters on everything. This is a good model in general, even if it’s just spending your money. But it’s also good to do for ethical reasons. Maybe this crosses the line from ethics to just general good software engineering practices. Most of the time it won’t have to do with ethics. You should put a rate limit on how often you ping someone. Not sure if that’s an ethics question, but maybe just a good idea.

This should prevent flash crash... very difficult to regulate.


## Limiting AI

You need to be constrain in some space.

If you're predicting handwritten letters and that's all your model's output, you are constrained in output. You can go wild - through away that boring MNIST and train it pictures of cats - it won't work but it won't hurt anyone.



## Domain Specific Constraints

Ethics will be different depending on the domain you’re in. Look at the difference between Steve Jobs and Elizabeth Holmes. They both had reality distortion fields around them that made them unable to hear things like “there’s no way the product will be ready in that timeline”. But the difference is when Jobs’s things broke, it wasn’t medical. That’s a *big* difference. That’s the difference between dealing with angry customers and criminal charges. There are huge implications from a false medical test. Same if you’re in the criminal justice field. Things *will* move slower in these fields and that’s a **good** thing (although they can also move far, far too slowly. I just mean compared to something with low downside risk, like cat v. dog detection, it will be behind). The most advanced medical technology will not be as advanced as the most advanced cat v dog detection technology (which, btw, is incredibly accurate). The same is true for medical equipment for humans vs mice. The rules are lower for mice, so we know more.

#### 



You have to be aware of what your model could possibly learn

If people are "liking" posts and your finding latent characteristics of those posts and promoting those with those characteristics, you don't know what you're promoting. You could be promoting something awful. There are things that aren't illegal that are still undesireable. Somewhere, there's a line on what constitutes child pornography. Just next to that line, on the legal side, is an image that's very close to illegal child pornography but isn't - perhaps it's a suggestive photograph or something like that. Would you want an algorithm to promote that? I don't think so. We shouldn't be promoting latent features that we don't understand. In some cases, the scope is constrained. If you are promoting types of apples to people based on their profiles, you're either going to promote a Granny Smith or a Honeycrisp, and there's no real harm from using variables you don't understand.

But if the scope is unlimited, like in unrestricted speech or images, we can't be promoting unknown features. The room for this to go bad is too great.

It's certainly true that models trained on machine learning are a reflection of the society that generated the training data. But just because it's a reflection doesn't mean it's OK to promote whatever the crowd would like.





there are architecture decisions that can result in more or fewer minority values showing up. l2 would have fewer, l1 would have more. but this is pretty far removed from the values-seems less valuable... do it with the data



#### Regulators

> Is Twitter the same as what you do?

- Senator Lindsey Graham question to Mark Zuckerberg

They have no idea what's going on. Five years ago he stated that he has never sent an email https://www.nbcnews.com/meet-the-press/lindsey-graham-ive-never-sent-email-n319571





articles:

https://onezero.medium.com/how-a-2018-research-paper-led-to-amazon-and-ibm-curbing-their-facial-recognition-programs-db9d6cb8a420
https://www.aclu.org/blog/privacy-technology/surveillance-technologies/amazons-face-recognition-falsely-matched-28?utm_campaign=The%20Batch&utm_medium=email&_hsmi=90161920&_hsenc=p2ANqtz-_zJ1t-xMsV0bHhUJYL5c9XqA27AcA1uQ32SuZ1SliqMZEYOOz7ECW7NQ1q-9qPpSt5mxO6O4lhhk7ngG5lknL4x2YFFWLJh1uUezENzF0Dnh9fiwU&utm_content=90161920&utm_source=hs_email
https://www.washingtonpost.com/technology/2019/05/16/police-have-used-celebrity-lookalikes-distorted-images-boost-facial-recognition-results-research-finds/?utm_campaign=The%20Batch&utm_medium=email&_hsmi=90161920&_hsenc=p2ANqtz-9ERgqghqdxYZ8YvP16iKYDVpMxdZXFZihQBweju7EWlQSpYknLOigIge4CHPlsv7iFmxovuMax6NaEkHxSasd-o1LManEbmTlaLTvmnDxDABaw59c&utm_content=90161920&utm_source=hs_email

https://www.latimes.com/local/lanow/la-me-ln-calgangs-audit-20160811-snap-story.htmlhttps://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsyhttps://www.propublica.org/article/facebook-enabled-advertisers-to-reach-jew-haters

